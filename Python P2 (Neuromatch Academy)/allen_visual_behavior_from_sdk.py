# -*- coding: utf-8 -*-
"""Allen Visual Behavior from SDK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-yIFdOPxC175rvzlVezQMawx7bNb8Nh-
"""



"""# Overview and Analysis Steps
---
The following notebook is based on an Allen Institute (AI) SDK sample notebook, which demonstrates how to load neural data for all imaging planes in one 2-photon imaging session into a single 'tidy' dataframe, make simple event-triggered plots, and do some basic analysis using scikit-learn. It was designed to demonstrate a simple method for interacting with the visual behavior data. Many aspects of the dataset are not explored here.

---

Here, we aim to show trajectories and derive attractors of VIP cells, which provide insight into memory processing dynamics in the visual cortex. The analysis uses data from the AI 2-photon visual behaviour pipeline. We selected  sessions with trained mice, in which familiar and novel images, and omissons were presented. The mice's behaviours (running) and arousal states (pupil size) were captured as well.

![text928-9-3-1.png](data:image/png;base64,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)

---

**Our analysis has following steps:**
0. Data selection: i) single session, ii) multiple sessions
1. Extract time intervals, 1.25s before and 1.5s after (stimulus is presented at time 0 for 500ms, the mice have 750ms to respond)
2. i) Average over all the trials for one image ID in one session (to get the hang of the required analysis steps), ii) distinguish various experimental conditions, e.g., arousal states and running, iii) for novel images distinguish between early (unfamiliar), mid (somehow familiar), and late trials (familiar)   
3. Create a heat map to visualize the resposes (an optional step to visualise the outcomes up to this point, another way to visualise the results can be used)
4. Convolve the timeseries with a gaussian kernel, more or less 200ms width (the wider the smoother) (width needs to be tested)
5. Put the smoothed timeseries in a matrix, each row is a cell, each column is a time point (might need to substract the mean from each row to remove the baseline response - this was actually already done in the data)
6. Compute the covariance matrix (1/T) * X * X^T (the result is cell x cell)
7. Compute the eigenvalues/eigenvectors of such matrix.
8. Take the first 2 or 3 eigenvectors corresponding to the highest eigenvalues, and project the original matrix X onto them (Z = V^TX)
9. Z should be now 3xTime, Plot Z as a curve, plot3D( Z(1,:), Z(2,:), Z(3,:)

# Set up environment and import packages

We have built a package called `mindscope_utilities` which contains some useful convenience functions. The `allenSDK` is a dependency of this package and will be automatically installed when you install `mindscope_utilities` per the instrutions below.

We will first install `mindscope_utilities` into our colab environment by running the commands below. When this cell is complete, click on the `RESTART RUNTIME` button that appears at the end of the output. Note that running this cell will produce a long list of outputs and some error messages. Clicking `RESTART RUNTIME` at the end will resolve these issues.

You can minimize the cell after you are done to hide the output.

#### Install necessary packages
"""

!python -m pip install --upgrade pip
!pip install mindscope_utilities

"""#### Next we will import packages we need later in the notebook"""

import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import seaborn as sns
import matplotlib.pyplot as plt

import sklearn as sk
from sklearn.decomposition import PCA

import mindscope_utilities
import mindscope_utilities.visual_behavior_ophys as ophys

from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache

pd.set_option('display.max_columns', 500)

"""# Load the session and experiment summary tables

The AllenSDK provides functionality for downloading tables that describe all sessions and experiments (individual imaging planes) in the Visual Behavior 2P dataset. We first download the data cache:
"""

data_storage_directory = "/temp" # Note: this path must exist on your local drive
cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_storage_directory)

"""- `Ophys_session_table` contains metadata describing imaging sessions. If more than one plane was imaged during a session, one ophys session id will be associated multiple ophys experiment ids. Each ophys session id will also have a unique behavior session id.
- `Behavior_session_table` contains metadata describing behavioral sessions, which may or may not be during imaging. Behavior session ids that do not have ophys session ids were training sessions.
- `Ophys_experiment_table` contains metadata describing imaging experiments (aka imaging planes). When mesoscope is used, one ophys session may contain up to 8 unique experiments (two visual areas by four imaging depths). Some imaging planes may not be released due to quality control issues, thus each ophys session id is associated with anywhere from one to eight unique experiment ids. Ophys experiment ids are unique and do not repeat across sessions. To find the same imaging plane that was matched across multiple sessions, use the `ophys_container_id` column that can be found in both `ophys_session_table` and `ophys_experiment_table`.

Then we can access the session and experiment tables directly.

Note that a 'session' is a single behavioral session. Sessions that are performed on the mesoscope will have multiple (up to 8) 'experiments' associated with them, where an experiment is a distinct imaging plane.
"""

session_table = cache.get_ophys_session_table()
experiment_table = cache.get_ophys_experiment_table()

"""We can then view the contents of the session table. Note that this contains a lot of useful metadata about each session. One of the columns, `ophys_experiment_id` provides a list of the experiments (aka imaging planes) that are associated with each session."""

session_table.project_code.unique()

"""The experiment table has one row per experiment. Note that the `ophys_session_id` column links each experiment to its associated session in the session_table."""

experiment_table.head()

"""# Load one example session
We are going to select one session from this table, session 854060305. This is a session with Sst-IRES-Cre mouse, which expressed GCaMP6f in Sst+ inhibitory interneurons. There were 6 simultaneously acquired imaging planes for this session.
We can view metadata for this session as follows:
"""

experiment_table.project_code.unique()

#lets focus on one project code
Visual_Behavior = experiment_table[experiment_table.project_code =="VisualBehavior"]

# focus on active training
visual_behavior_Active = Visual_Behavior[~Visual_Behavior.session_type.str.contains('passive', na=False,)]

# there are 35 mouse (session) for visual behavior active sessions
len(visual_behavior_Active.mouse_id.unique())

len(visual_behavior_Active.ophys_session_id.unique())

# focus on Vip cells
visual_behavior_Active_Vip = visual_behavior_Active[visual_behavior_Active.cre_line == "Vip-IRES-Cre"]



## exposure to image set from omission list, check omission number 5
for i in range(13):
  print(visual_behavior_Active_Vip[visual_behavior_Active_Vip.prior_exposures_to_omissions==i].prior_exposures_to_image_set,i)



# (Pramod) toggle for splitting novel/stimulus  experiment_id
toggle = 10 # toggle should control level of novel vs familiar
exp_list_familiar = list(visual_behavior_Active_Vip[visual_behavior_Active_Vip.prior_exposures_to_image_set>=toggle].index)
exp_list_novel = list(visual_behavior_Active_Vip[visual_behavior_Active_Vip.prior_exposures_to_image_set<toggle].index)

len(exp_list_novel)



# (Pramod) get the data for the experiment
experiments = {}
ophys_experiment_ids = []

for ophys_experiment_id in visual_behavior_Active_Vip.index[0]:
    experiments[ophys_experiment_id] = cache.get_behavior_ophys_experiment(ophys_experiment_id)

# (Pramod) splitting original experimental data into two dictionaries, novel and famiiar
filterByKey = lambda keys: {x: experiments[x] for x in keys}
exp_list_familiar_data = filterByKey(exp_list_familiar)
exp_list_novel_data = filterByKey(exp_list_novel)

print(exp_list_familiar_data[exp_list_familiar[0]])

# (Pramod) Loading neuronal data of our experiment of interest's data into memory (first familiar data)
def neural_data_loading(exp_list_data,exp_id):
  neural_data = []
  this_experiment = exp_list_data[exp_id]
  this_experiment_neural_data = ophys.build_tidy_cell_df(this_experiment)

  # add some columns with metadata for the experiment
  metadata_keys = [
      'ophys_experiment_id',
      'ophys_session_id',
      'targeted_structure',
      'imaging_depth',
      'equipment_name',
      'cre_line',
      'mouse_id',
      'sex',
  ]
  for metadata_key in metadata_keys:
      this_experiment_neural_data[metadata_key] = this_experiment.metadata[metadata_key]

  # append the data for this experiment to a list
  neural_data.append(this_experiment_neural_data)

  # concatate the list of dataframes into a single dataframe
  neural_data = pd.concat(neural_data)

  return neural_data

# (Pramod) Now neural_data_familiar and neural_data_novel will be filled by dataframes of first data point each through above function

# (Pramod) Passing first element of list to get a neural data frame
neural_data_familiar = neural_data_loading(exp_list_familiar_data,exp_list_familiar[0])

neural_data_novel = neural_data_loading(exp_list_novel_data,exp_list_novel[0])

# (Pramod) get the data for the experiment, passing first novel and famiiar id
experiment_data_familiar = cache.get_behavior_ophys_experiment(exp_list_familiar[0])
experiment_data_novel = cache.get_behavior_ophys_experiment(exp_list_novel[0])

# (Pramod) Query Function
def get_neural_data_query(neural_data,event):
  #print (event)
  etr = []
  for cell_id in list(neural_data.cell_specimen_id.unique()):
    etr.append(mindscope_utilities.event_triggered_response(
      data = neural_data.query('cell_specimen_id == @cell_id'),
      t = 'timestamps',
      y = 'filtered_events',
      event_times = stimulus_table.query(event)['start_time'],
      t_before=3,
      t_after=3,
      output_sampling_rate = 50,
  ))
  return etr

etr_familiar

### getting error here need to resolve !!!!!

# (Pramod) Returning etr neural data for familiar/novel neurons
event = "is_changed"
etr_familiar = get_neural_data_query(neural_data_familiar,event)
etr_novel = get_neural_data_query(neural_data_novel,event)

# (Pramod) Working on Matrix
def get_matrix_data(etr):
  Final_data = []
  for i in range(len(etr)):

    a = [d for _, d in etr[i].groupby(['event_number'])]
    Final_data.append([list(I.filtered_events) for I in a])   #you can change the filtered events by dff


  Final_data = np.array(Final_data)
  Final_data = Final_data.transpose(0,2,1)
  print (Final_data.shape)
  return Final_data

# (Pramod) Printing Final Data Matrix
final_data_familiar = get_matrix_data(etr_familiar)
final_data_novel = get_matrix_data(etr_novel)

chosen_experiment_id =830093338

#get the data for the experiment
experiment_data = cache.get_behavior_ophys_experiment(chosen_experiment_id)

#see the information of the experiment
experiment_data.metadata

# Loading neuronal data of our experiment of interest's data into memory

neural_data = []
this_experiment = experiment_data
this_experiment_neural_data = ophys.build_tidy_cell_df(this_experiment)

# add some columns with metadata for the experiment
metadata_keys = [
    'ophys_experiment_id',
    'ophys_session_id',
    'targeted_structure',
    'imaging_depth',
    'equipment_name',
    'cre_line',
    'mouse_id',
    'sex',
]
for metadata_key in metadata_keys:
    this_experiment_neural_data[metadata_key] = this_experiment.metadata[metadata_key]

# append the data for this experiment to a list
neural_data.append(this_experiment_neural_data)

# concatate the list of dataframes into a single dataframe
neural_data = pd.concat(neural_data)

neural_data

#get the stimulus table for the experiment

stimulus_table = experiment_data.stimulus_presentations

stimulus_table

# There are 22 unique cells
cell_ids = neural_data['cell_specimen_id'].unique()
print('there are {} unique cells'.format(len(cell_ids)))
print('cell ids are: {}'.format(cell_ids))

neural_data.columns

"""Plotting one neuronal time series"""

single_cell_timeseries = neural_data.query('cell_specimen_id ==  '+str(list(cell_ids)[0])+'')

neurona

fig, ax = plt.subplots(figsize=(15,5))
single_cell_timeseries.plot(
    x = 'timestamps',
    y = 'dff',
    ax = ax
)

fig, ax = plt.subplots(figsize=(15,5))
single_cell_timeseries.plot(
    x = 'timestamps',
    y = 'filtered_events',
    ax = ax
)

"""Get trials dataset"""

#is_change , omitted

etr = []
for cell_id in list(neural_data.cell_specimen_id.unique()):
  etr.append(mindscope_utilities.event_triggered_response(
    data = neural_data.query('cell_specimen_id == @cell_id'),
    t = 'timestamps',
    y = 'filtered_events',
    event_times = stimulus_table.query('omitted')['start_time'],
    t_before=3,
    t_after=3,
    output_sampling_rate = 50,
))
etr[1]





"""#**omitted**"""

















"""Get the following array:

# Neurons * Trial_time * Number_of_trials
"""

Final_data = []
for i in range(len(etr)):

  a = [d for _, d in etr[i].groupby(['event_number'])]
  Final_data.append([list(I.filtered_events) for I in a])   #you can change the filtered events by dff


Final_data = np.array(Final_data)
Final_data = Final_data.transpose(0,2,1)
print (Final_data.shape)

Final_data_changed=Final_data

Final_data_omitted=Final_data









np.shape(a)

"""Stack the whole data into:

Trial * (Neuron*trace)
"""

Final_data = Final_data.transpose(0,2,1)
np.shape(Final_data)



a = Final_data.transpose(1,0,2)


stacked_data=np.hstack(a)
np.shape(stacked_data)

plt.plot(np.mean(stacked_data,axis=0))

np.shape(stacked_data)

b=[]

for i in range(len(stacked_data)):
  b.append(np.convolve(stacked_data[i,:],np.ones(30)/30, mode='valid'))




pca = PCA(n_components=2)
pca.fit(np.array(b).T)
pca_stacked_data = pca.transform(np.array(b).T)
np.shape(pca_stacked_data)



for i in range(np.shape(Final_data)[2]):
  y=pca_stacked_data[(i)*301:(i+1)*301,:]



  plt.plot(y[:,0],y[:,1], color=(i/200.0,i/200.0,i/200.0))


plt.plot(x[:,0],x[:,1],linewidth=2)



np.shape(Final_data)[2
                     ]

for i in range(np.shape(Final_data)[2]):
  y=pca_stacked_data[(i)*301:(i+1)*301,:]



  plt.plot(y[:,0],y[:,1], color=(i/200.0,i/200.0,i/200.0))


plt.plot(x[:,0],x[:,1],linewidth=2)

print(pca.explained_variance_ratio_)

fig = plt.figure()
#ax = fig.add_subplot(projection='3d')

plt.plot(pca_stacked_data[:,0],pca_stacked_data[:,1])
#ax.plot3D(x[np.shape(x)[0]//2, 0],x[np.shape(x)[0]//2, 1],x[np.shape(x)[0]//2, 2],'o')
print('First:',x[0])
print('Last:',x[-1])
#ax.plot3D(x[0,0],x[0,1],x[0,2],'s')



fig = plt.figure()
#ax = fig.add_subplot(projection='3d')

plt.plot(x[:,0],x[:,1])
#ax.plot3D(x[np.shape(x)[0]//2, 0],x[np.shape(x)[0]//2, 1],x[np.shape(x)[0]//2, 2],'o')
print('First:',x[0])
print('Last:',x[-1])
#ax.plot3D(x[0,0],x[0,1],x[0,2],'s')







#individual PCA over each trial / full pca
#density of color increases over trial time
my_cmap = sns.light_palette("Navy", as_cmap=True)
colors = np.linspace(0,1,176)

#fig = plt.figure()
#ax = fig.add_subplot(projection='3d')
for i in range(len(Final_data)):

  pca = PCA(n_components=2)
  pca.fit(Final_data[i])

  x = pca.transform(Final_data[i])
  #plt.scatter(x[:,0],x[:,1], label=i)
  #plt.show()
  z=x/pca_stacked_data
  plt.scatter(z[:,0],z[:,1], c=colors, cmap=my_cmap)

plt.colorbar()
plt.clim(0,176)
#plt.legend()

"""## **individual PCA over each trial**"""

np.shape(Final_data)

Final_data=Final_data.transpose(0,2,1)
  np.shape( Final_data)

#individual PCA over each trial
my_cmap = sns.light_palette("Navy", as_cmap=True)
colors = np.linspace(0,1,len(Final_data[1]))

#fig = plt.figure()
#ax = fig.add_subplot(projection='3d')
for i in range(len(Final_data)):

  pca = PCA(n_components=2)
  pca.fit(Final_data[i])

  x = pca.transform(Final_data[i])
  #plt.scatter(x[:,0],x[:,1], label=i)
  #plt.show()

  plt.plot(x[:,0],x[:,1])
#lt.colorbar()
#plt.clim(0,len(Final_data[1]))
#plt.legend()

q=np.mean(  Final_data,axis=0)
pca = PCA(n_components=2)
pca.fit(q)

x = pca.transform(q)

plt.plot(x[:,0],x[:,1])

np.shape(x)

"""## **average over trials**

---


"""

np.shape(v)

a=[]
v=np.mean(Final_data,axis=2)
for i in range(len(v)):
  a.append(np.convolve(v[i,:],np.ones(30)/30, mode='valid'))


plt.imshow(a)

plt.plot(np.mean(v,axis=0))
plt.plot(np.mean(a,axis=0))

np.shape(Final_data)

pca = PCA(n_components=2)
pca.fit(np.array(a).T)
x = pca.transform(np.array(a).T)


print(pca.explained_variance_ratio_)



fig = plt.figure()
#ax = fig.add_subplot(projection='3d')

plt.plot(x[:,0],x[:,1])
#ax.plot3D(x[np.shape(x)[0]//2, 0],x[np.shape(x)[0]//2, 1],x[np.shape(x)[0]//2, 2],'o')
print('First:',x[0])
print('Last:',x[-1])
#ax.plot3D(x[0,0],x[0,1],x[0,2],'s')

np.shape(x)[0]

plt.plot(x[:,0])

plt.plot(x[:,1])

z=np.mean(Final_data_changed,axis=2)
q=np.mean(Final_data_omitted,axis=2)

data=np.hstack((z,q))

np.shape(data)

a=[]
for i in range(len(z)):
  a.append(np.convolve(z[i,:],np.ones(30)/30, mode='valid'))


b=[]
for i in range(len(q)):
  b.append(np.convolve(q[i,:],np.ones(30)/30, mode='valid'))

conv=np.hstack((a,b))

pca = PCA(n_components=3)
pca.fit(np.array(conv).T)
x = pca.transform(np.array(conv).T)

m=np.shape(x)[0]
m

fig = plt.figure()
ax = fig.add_subplot(projection='3d')

ax.plot(x[:m//2,0],x[:m//2,1],x[:m//2,2])

ax.plot(x[m//2+1:,0],x[m//2+1:,1],x[m//2+1:,2])
ax.plot([x[m//4, 0]],[x[m//4, 1]],[x[m//4, 2]],'o')
ax.plot([x[3*m//4, 0]],[x[3*m//4, 1]],[x[3*m//4, 2]],'o')
ax.plot([x[0, 0]],[x[0, 1]],[x[0, 2]],'o')
ax.plot([x[m//2, 0]],[x[m//2, 1]],[x[m//2, 2]],'o')
#plt.plot(x[0,0],x[0,1],'s')



"""#omitted"""

a=[]
v=np.mean(Final_data,axis=2)
for i in range(len(v)):
  a.append(np.convolve(v[i,:],np.ones(30)/30, mode='valid'))


plt.imshow(a)

plt.plot(np.mean(v,axis=0))
plt.plot(np.mean(a,axis=0))

pca = PCA(n_components=2)
pca.fit(np.array(a).T)
x = pca.transform(np.array(a).T)


print(pca.explained_variance_ratio_)

fig = plt.figure()
#ax = fig.add_subplot(projection='3d')

plt.plot(x[:,0],x[:,1])
plt.plot(x[np.shape(x)[0]//2, 0],x[np.shape(x)[0]//2, 1],'o')
print('First:',x[0])
print('Last:',x[-1])
plt.plot(x[0,0],x[0,1],'s')

b=[]

for i in range(len(stacked_data)):
  b.append(np.convolve(stacked_data[i,:],np.ones(30)/30, mode='valid'))




pca = PCA(n_components=2)
pca.fit(np.array(b).T)
pca_stacked_data = pca.transform(np.array(b).T)
np.shape(pca_stacked_data)



for i in range(np.shape(Final_data)[2]):
  y=pca_stacked_data[(i)*301:(i+1)*301,:]



  plt.plot(y[:,0],y[:,1], color=(i/200.0,i/200.0,i/200.0))


plt.plot(x[:,0],x[:,1],linewidth=2)





fig = plt.figure()
ax = fig.add_subplot(projection='3d')

ax.scatter(x[:,0],x[:,1],x[:,2])

















[]

"""# Download all associated experiments

Each session consists of one or more 'experiments', in which each experiment is a single imaging plane

Each mesoscope session has up to 8 experiments associated with the session. We will load all sessions into a dictionary with the experiment IDs as the keys

The first time that this cell is run, the associated NWB files will be downloaded to your local `data_storage_directory`. Subsequent runs of this cell will be faster since the data will already be cached locally.
"""

experiments = {}
ophys_experiment_ids = session_table.loc[ophys_session_id]['ophys_experiment_id']
for ophys_experiment_id in ophys_experiment_ids:
    experiments[ophys_experiment_id] = cache.get_behavior_ophys_experiment(ophys_experiment_id)

experiments

"""## View the max projection and one cell ROI for one of the experiments
We can view the `cell_specimen_table` for one experiment, which contains information about each identified cell in that experiment
"""

experiment = experiments[ophys_experiment_ids[1]]
experiment.cell_specimen_table.head()

"""We can then visualize the max projection and one of the identified ROIs"""

fig, ax = plt.subplots(1, 2, figsize=(15,8), sharex=True, sharey=True)
ax[0].imshow(experiment.max_projection, cmap='gray')
ax[0].set_title('max projection')

cell_specimen_id = experiment.cell_specimen_table.index[2]
ax[1].imshow(experiment.cell_specimen_table.loc[cell_specimen_id]['roi_mask'])
ax[1].set_title('ROI mask for cell_specimen_id = {}'.format(cell_specimen_id))

"""## Load neural data into memory

The cell below will load the neural data into memory in the pandas 'tidy' format by iterating over each of the 6 experiments and using some helpful tools from the `visual_behavior_ophys` module of the `mindscope_utilities` package that was imported above as `ophys`.

It will also include a subset of metadata from `ophys_experiment_table` to facilitate splitting by depth, structure (aka cortical area), cre line (aka cell class), etc.

Note that 'tidy' data means that each row represents only one observation. Observations are stacked vertically. Thus, the `timestamps` colums will repeat for every cell in the dataset.
"""

neural_data = []
for ophys_experiment_id in tqdm(experiments.keys()): #tqdm is a package that shows progress bars for items that are iterated over
    this_experiment = experiments[ophys_experiment_id]
    this_experiment_neural_data = ophys.build_tidy_cell_df(this_experiment)

    # add some columns with metadata for the experiment
    metadata_keys = [
        'ophys_experiment_id',
        'ophys_session_id',
        'targeted_structure',
        'imaging_depth',
        'equipment_name',
        'cre_line',
        'mouse_id',
        'sex',
    ]
    for metadata_key in metadata_keys:
        this_experiment_neural_data[metadata_key] = this_experiment.metadata[metadata_key]

    # append the data for this experiment to a list
    neural_data.append(this_experiment_neural_data)

# concatate the list of dataframes into a single dataframe
neural_data = pd.concat(neural_data)

"""We can then look at some attributes of the `neural_data` dataframe we have created.

It is ~2.5 million rows long:
"""

len(neural_data)

"""It is so long because has one row for each timestamp for each cell.

Below are the first 5 entries. Again, note that the `tidy` format means that each row has only one observation, which represents a single GCaMP6 fluorescnce value for a single neuron.
"""

neural_data.head()

"""- The `cell_roi_id` column contains unique roi ids for all cells in a given experiment, which do not repeat across ophys sessions.
- The `cell_specimen_id` column contains unique ids for cells that were matched across ophys sessions. Thus, a cell that was imaged in more than one session has multiple roi ids but one cell specimen id.

# Examine Cell IDs
We can get the unique Cell IDs in our dataset as follows:
"""

cell_ids = neural_data['cell_specimen_id'].unique()
print('there are {} unique cells'.format(len(cell_ids)))
print('cell ids are: {}'.format(cell_ids))

"""If we wanted to get the timeseries for one cell, we could query the `neural_data` dataframe. For example, to get the full timeseries for the cell with `cell_specimen_id = 1086557208`:"""

single_cell_timeseries = neural_data.query('cell_specimen_id == 1086557208')
single_cell_timeseries.head()

"""Each cell has three types of traces:
- `dff` column is the Calcium fluorescence signal, normalized to background fluorescence.
- `events` column is deconvolved events from dff trace, which approximates neural firing rate and removes the slow decay of the Calcium signal (for more details, you can read EVENT DETECTION section in [Visual Behavior whitepaper](https://portal.brain-map.org/explore/circuits/visual-behavior-2p)).
- `filtered_events` column is events smoothed with a half-gaussian kernel.

We can then plot DeltaF/F for this cell for the full experiment as follows:
"""

fig, ax = plt.subplots(figsize=(15,5))
single_cell_timeseries.plot(
    x = 'timestamps',
    y = 'dff',
    ax = ax
)

"""# Load stimulus data into memory
The stimulus table is shared across all experiments (imaging planes) in a session. We can therefore use the stimulus table for just one experiment.

We are going to drop the `image_set` column because it is not informative for our purposes. We can then view the first 10 rows of the stimulus table.
"""

stimulus_table = experiments[ophys_experiment_ids[0]].stimulus_presentations.drop(columns = ['image_set']) # dropping the 'image_set' column to avoid confusion. Image_set column contains a unique string for set of images presented in a session.
stimulus_table.head(10)

"""## View the `stimulus_templates` attribute
Note that the `unwarped` column contains the image before the application of a spherical warp. All of the pixels labeled 'NaN' will be off-screen (not visible to the mouse) after the warp is applied.

All experiments in a given session will share the same `stimulus_templates`
"""

experiment = experiments[ophys_experiment_ids[0]]
experiment.stimulus_templates

"""## View the unwarped images"""

fig, ax = plt.subplots(2,4,figsize = (20,8), sharex = True, sharey=True)
for ii,image_name in enumerate(experiment.stimulus_templates.index):
    ax.flatten()[ii].imshow(experiment.stimulus_templates.loc[image_name]['unwarped'], cmap='gray')
    ax.flatten()[ii].set_title(image_name)
fig.tight_layout()

"""## View the warped images
This represents what was actually on the screen during the session
"""

fig, ax = plt.subplots(2,4,figsize = (20,8), sharex = True, sharey=True)
for ii,image_name in enumerate(experiment.stimulus_templates.index):
    ax.flatten()[ii].imshow(experiment.stimulus_templates.loc[image_name]['warped'], cmap='gray')
    ax.flatten()[ii].set_title(image_name)
fig.tight_layout()

"""## Describe stimulus omissions
An important feature of the task is that stimuli are shown at a very regular cadence (250 ms on, 500 ms off), but stimuli are randomly omitted with a probability of ~5%. These unexpected and random stimulus omissions could be perceived as an expectation violation by the mouse.

Omitted stimuli are denoted in the `stimulus_table` by the `omitted` column. `True` means that the stimulus that would have been shown at that time was actually omitted (and was replaced by an extended gray screen between stimuli).

We can look at the first 10 examples of omitted stimuli as follows. Note that each 'omitted' stimulus still has a 'start_time' and a 'stop_time' associated with it. This actually represents the time that a stimulus would have been shown, had it not been omitted.

Stimulus omissions are also indicated in the `image_name` column by the string `omitted`
"""

stimulus_table.query('omitted').head(10)

"""# Create an event triggered response dataframe relative to omissions
If we want to see how a given cell responds when regularly flashed stimuli are omitted, we can calculate the response around each of the stimulus omissions. The `mindscope_utilities` package has a convenience function to do this. We give the function:
* a dataframe of interest (containing activity from one cell)
* the t and y values of interest
* the event times
* how much time before and after each event we are interested in
* the desired sampling rate of the output - this is the rate onto which the response will be interpolated

The function will return a new dataframe with the response for the given cell, aligned to each of the events.
"""

cell_id = cell_ids[11]
etr = mindscope_utilities.event_triggered_response(
    data = neural_data.query('cell_specimen_id == @cell_id'),
    t = 'timestamps',
    y = 'dff',
    event_times = stimulus_table.query('omitted')['start_time'],
    t_before=3,
    t_after=3,
    output_sampling_rate = 50,
)
etr

"""We can see that the output has colums for
* `time` - this is our new timebase relative to the events. In this case, it ranges from -3 to 3
* `dff` - this is the deltaF/F value surrounding each event, interpolated onto the new timebase. If, when calling the `event_triggered_response` function we had passed `y = 'events'`, this column would be events instead of dff.
* `event_number` - this is an integer representing the count of each event. In this example, there were 185 omissions, so they are numbered from 0 to 184
* `event_time` - this is the time of each event

The docstring for the `event_triggered_response` function can be viewed as follows:
"""

mindscope_utilities.event_triggered_response?

"""## Plot an event triggered response

The output format of the `event_triggered_response` function is designed to plug directly into Seaborn's `lineplot` plotting function. We can then view the mean response to omitted stimuli with 95% confidence intervals very easily:
"""

sns.lineplot(
    data=etr,
    x='time',
    y='dff',
    n_boot=500
)

"""Note that the regular, image-driven responses with a 750 ms inter-stimulus interval are visible everywhere except at t=0, which is when the unexpectedly omitted stimulus occured.

### Make a function to plot an event triggered average in one line

If we make a wrapper function that combines the process of calculating and plotting the event triggered response, it can be called in a single line below. By having `event_query` input variable, we can use this function to plot responses to any event of interest (omisisons, changes, hits/misses, specific images, etc)
"""

def make_event_triggered_plot(df, x, y, event_query, ax, t_before=3, t_after=3):
    etr = mindscope_utilities.event_triggered_response(
        data = df,
        t = 'timestamps',
        y = y,
        event_times = stimulus_table.query(event_query)['start_time'],
        t_before=t_before,
        t_after=t_before,
        output_sampling_rate = 50,
    )
    sns.lineplot(
        data=etr,
        x=x,
        y=y,
        n_boot=500,
        ax=ax
    )

"""Now plot the omission triggered response for the same cell using filtered events (these events extracted from the deltaF/F timeseries using an event extraction algorithm, then smoothed with a half-gaussian kernel) instead of dff."""

cell_id = cell_ids[11]
fig, ax = plt.subplots()
make_event_triggered_plot(
    df = neural_data.query('cell_specimen_id == @cell_id'),
    x = 'time',
    y = 'filtered_events',
    event_query = 'omitted',
    ax=ax
)

"""## Plot the responses for 10 sample cells
We can then iterate over 10 randomly chosen cells and plot their activity during omissions.
"""

np.random.seed(0)
fig, ax = plt.subplots()
for cell_id in tqdm(np.random.choice(cell_ids, size=10, replace=False)):

    make_event_triggered_plot(
        df = neural_data.query('cell_specimen_id == @cell_id'),
        x = 'time',
        y = 'dff',
        event_query = 'omitted',
        ax=ax
    )

"""Interestingly, not all SST cells in this session do the same thing!

## Calculate the mean response for each of the individual imaging planes in this experiment
By iterating over experiment IDs, we can also calculate the mean response for each of the 6 imaging planes. Do Sst cells in different visual areas respond to omissions in a distinct way?

We will first use a Pandas `groupby` and `mean` operations to get the mean timeseries for each cell in that imaging plane:
"""

mean_dff_by_experiment = (
    neural_data
    .groupby(['ophys_experiment_id','timestamps'])['dff']
    .mean()
    .reset_index()
)

mean_dff_by_experiment.head()

"""We can then iterate over our 6 experiment IDs and use our `make_event_triggered_plot` wrapper function to calculate and plot the omission triggered response for that imaging plane:"""

# set up a new figure and axis
fig, ax = plt.subplots()

# make an empty list that we will fill with strings for the legend
legend_text = []

# iterate over every `ophys_experiment_id`
for ophys_experiment_id in tqdm(ophys_experiment_ids):

    make_event_triggered_plot(
        df = mean_dff_by_experiment.query('ophys_experiment_id == @ophys_experiment_id'),
        x = 'time',
        y = 'dff',
        event_query = 'omitted',
        ax=ax
    )

    # get some metadata to add to the legend
    this_exp = neural_data.query('ophys_experiment_id == @ophys_experiment_id')
    structure = this_exp['targeted_structure'].iloc[0]
    depth = this_exp['imaging_depth'].iloc[0]

    # append a string to our list of legend text
    legend_text.append('structure = {}\ndepth = {} um'.format(structure, depth))

# Put the legend out of the figure
plt.legend(legend_text, bbox_to_anchor=(1.05, 1))

"""There are clearly some large differences in the way that Sst cells respond to these unexpected stimulus omissions by area.

This example could be extended to include cells from the other two cre-lines in the dataset: The VIP-Cre line which labels VIP+ inhibitory interneurons and the Slc17a7 line, which is a pan-excitatory line.
"""

session_table['cre_line'].unique()

"""In addition, responses to different stimuli could be explored, along with responses relative to other behavioral measures, such as licking.

For a full description of the dataset and all available data streams, see the Visual Behavior Project Description at:
https://portal.brain-map.org/explore/circuits/visual-behavior-2p

# Set up data for scikit learn
What if we wanted to use scikit-learn for a decoding or clustering analysis? We'd need to get the data into a standard format for scikit learn, which is often a feature matrix (`X`) and a vector of labels (`y`).

Instead of just omissions, let's now look at the responses to each of the stimuli in this session, which consists of 8 unique images, plus the omitted stimuli (which we characterize as a unique stimulus type). First, we will calculate an event triggered response to each stimulus start time in the stimulus table.
"""

full_etr = []
# iterate over each unique cell
for cell_specimen_id in tqdm(neural_data['cell_specimen_id'].unique()):
    # calculate the event triggered response for this cell to every stimulus
    full_etr_this_cell = mindscope_utilities.event_triggered_response(
        neural_data.query('cell_specimen_id == @cell_specimen_id'),
        t = 'timestamps',
        y = 'dff',
        event_times = stimulus_table['start_time'],
        t_before = 0,
        t_after = 0.75,
        output_sampling_rate = 30
    )
    # add a column identifying the cell_specimen_id
    full_etr_this_cell['cell_specimen_id'] = cell_specimen_id
    # append to our list
    full_etr.append(full_etr_this_cell)

# concatenate our list of dataframes into a single dataframe
full_etr = pd.concat(full_etr)

# cast these numeric columns to int and float, respectively
full_etr['event_number'] = full_etr['event_number'].astype(int)
full_etr['event_time'] = full_etr['event_number'].astype(float)
# rename 'event_number' as
full_etr.rename(columns = {'event_number': 'stimulus_presentations_id'}, inplace=True)

"""One way to construct a feature matrix might be to build it such that dimensions are `trials x cells`. Thus:

* Each row would be one trial, where a trial is defined as a unique image presentation
* Each column would represent the average response of a given cell on that image presentation.

To do so, let's construct another intermediate dataframe called `average_responses` that contains the average response of each cell (in the 750 ms window we've selected above) to each image presentation. We'll do this using a Pandas groupby to group by `cell_specimen_id` and `stimulus_presentations_id` (aka trial).

We're also going to merge in our stimulus metadata.
"""

average_responses = full_etr.groupby(['cell_specimen_id','stimulus_presentations_id'])[['dff']].mean().reset_index().merge(
    stimulus_table,
    on = 'stimulus_presentations_id',
    how = 'left'
)
average_responses

"""Now we can construct a dataframe called `features_and_labels` that will contain one row per trial, one column per cell, plus columns with the image_index and image_name"""

features_and_labels = average_responses.pivot(
    index = 'stimulus_presentations_id',
    columns = 'cell_specimen_id',
    values = 'dff'
).merge(
    stimulus_table[['image_index','image_name']],
    on = 'stimulus_presentations_id',
    how = 'left'
)
features_and_labels.sample(10)

"""The X matrix can be extracted by getting the columns associated with the cell_specimen_ids"""

X = features_and_labels[cell_ids]
X.sample(10)

"""And `y` is just the `image_name` column (it could also be the `image_index` column if you want a numeric value instead of a string to represent the image identity)"""

y = features_and_labels['image_name']
y.sample(10)

"""## Dimensionality reduction
Now we can use t-SNE, which will project our 53-dimensional feature space (53 neurons in the session) into two dimensions.
"""

from sklearn.manifold import TSNE

X_embedded = TSNE(n_components=2).fit_transform(X.values)

"""And visualize the results, with colors representing each unique stimulus."""

features_and_labels['tsne-2d-one'] = X_embedded[:,0]
features_and_labels['tsne-2d-two'] = X_embedded[:,1]
plt.figure(figsize=(16,10))
ax = sns.scatterplot(
    data=features_and_labels,
    x="tsne-2d-one",
    y="tsne-2d-two",
    hue="image_name",
    hue_order = np.sort(features_and_labels['image_name'].unique()),
    palette=sns.color_palette()[:9],
    legend="full",
    alpha=0.3
)

"""This demonstrates that the time-averaged population responses to at least some of the stimuli seem to fall into distinct clusters in our 53-dimensional space, while others appear more overlapped. This implies that a decoding analysis might be more successful at decoding some stimuli than others.

## Train a simple decoder
We can use an SVM decoder from scikit learn to ask how well we can decode image identity from the feature matrix we have constructed.
"""

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import confusion_matrix, accuracy_score

"""Split our data into train and test sets, instantiate the model, then fit."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
model = svm.SVC(probability=True)
model.fit(X_train, y_train)

"""Use the model to make predictions on the held-out test set"""

y_pred = model.predict(X_test)

"""Evaluate the accuracy"""

accuracy_score(y_test, y_pred)

"""Evaluate the confusion matrix"""

pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    columns = ['predicted_{}'.format(im) for im in model.classes_],
    index = ['actual_{}'.format(im) for im in model.classes_]
)

"""This tells us that the model can decode some stimuli well (im035, im075 and im106, for example), while it struggles more with others (im000 and omissions, for example). Do the stimuli that the decoder succeeds in classifying align with those that cluster cleanly in t-SNE space?

### Follow up exercise

Can you create event triggered averages and perform decoding using other events of interest, such as licks or rewards?
"""

# Lick and reward data are available for each experiment
licks = experiments[ophys_experiment_id].licks
licks.head()

rewards = experiments[ophys_experiment_id].rewards
rewards.head()

"""To see the full list of all attributes available for each experiment via the AllenSDK, uncomment the cell below and run it"""

# help(experiments[ophys_experiment_id])













